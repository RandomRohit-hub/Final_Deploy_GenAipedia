import streamlit as st
from dotenv import load_dotenv
import os

# -------------------- RAG Imports --------------------
from pinecone import Pinecone
from langchain_pinecone import PineconeVectorStore
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_groq import ChatGroq

from langchain.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain

# -------------------- Load .env --------------------
load_dotenv()

PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
GROQ_API_KEY = os.getenv("GROQ_API")

if not PINECONE_API_KEY:
    st.error("‚ùå Missing PINECONE_API_KEY in environment")
if not GROQ_API_KEY:
    st.error("‚ùå Missing GROQ_API in environment")

# -------------------- Pinecone Initialization (NEW SDK) --------------------
pc = Pinecone(api_key=PINECONE_API_KEY)

index_name = "genativeai-encyclopedia"   # your index name
namespace = ""                            # change if you used a namespace

# Check if index exists
if index_name not in pc.list_indexes().names():
    st.error(f"‚ùå Pinecone index '{index_name}' does not exist!")
else:
    st.success(f"Connected to Pinecone index: {index_name}")

# -------------------- Streamlit UI --------------------
st.set_page_config(page_title="GenAiPedia Chatbot", layout="wide")
st.title("ü§ñ GenAiPedia ‚Äî AI Knowledge Chatbot")
st.markdown("Ask anything related to AI/ML ‚Äî powered by your Pinecone Vector DB.")

# -------------------- Embeddings --------------------
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={"device": "cpu"}
)

# -------------------- Vector Store (Load Existing Index) --------------------
db = PineconeVectorStore.from_existing_index(
    index_name=index_name,
    embedding=embeddings,
    namespace=namespace
)

retriever = db.as_retriever(search_kwargs={"k": 10})

# -------------------- Groq Model --------------------
llm = ChatGroq(
    model="llama-3.1-8b-instant",
    temperature=0,
    api_key=GROQ_API_KEY
)

# -------------------- Prompt Template --------------------
system_prompt = """
You are a helpful AI assistant. Use ONLY the provided context to answer.
If the answer is not in the context, say: "I don't know."

Context:
{context}
"""

prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("human", "{input}")
])

# -------------------- Build RAG Chain --------------------
stuff_chain = create_stuff_documents_chain(llm, prompt)
rag_chain = create_retrieval_chain(retriever, stuff_chain)

# -------------------- Chat Input --------------------
user_input = st.text_input("Enter your question:")

if user_input:
    with st.spinner("Generating answer..."):
        result = rag_chain.invoke({"input": user_input})
        answer = result.get("answer", "I don't know.")

    st.subheader("üìò Answer")
    st.write(answer)

    # Debug / Retrieved context
    with st.expander("üîç Retrieved Knowledge Chunks"):
        for i, doc in enumerate(result.get("context", []), 1):
            st.markdown(f"### Chunk {i}")
            st.write(doc.page_content)
            st.caption(f"Source: {doc.metadata.get('source')}")
